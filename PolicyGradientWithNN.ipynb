{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PolicyGradientWithNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwBjdH5suVTX"
      },
      "source": [
        "# import argparse\n",
        "import gym\n",
        "import numpy as np\n",
        "from itertools import count #  iterators which helps in getting faster execution time and writing memory-efficient code\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn # used for nn.Linear, nn.dropout\n",
        "import torch.nn.functional as F # used for F.relu, F.softmax\n",
        "import torch.optim as optim # used for optim.Adam\n",
        "from torch.distributions import Categorical # used to create a Categorical distribution with our actions and their probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ybZzWAvfvW"
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "OBSERVATION_SPACE = env.observation_space.shape[0] \n",
        "# 4 for CartPole-v1: Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity\n",
        "\n",
        "ACTION_SPACE = env.action_space.n\n",
        "# 2 for CartPole-v1: move Left or Right\n",
        "\n",
        "DROPOUT_RATE = 0.6\n",
        "# choose a dropout rate\n",
        "\n",
        "HIDDEN_LAYER_SIZE = 128\n",
        "# choose a size for the hidden layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOFh-uV5vhHN"
      },
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99\n",
        "render = False\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    # Our policy will be a neural network with 4 inputs (the environment info),\n",
        "    # 128 neurons in the hidden layer (w/ Dropout at 60%) and 2 outputs\n",
        "    # (corresponding to the two possible actions: left or right)\n",
        "    # a rectified linear unit activation function and a softmax are also part\n",
        "    # of the process\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.layer1 = nn.Linear(OBSERVATION_SPACE, HIDDEN_LAYER_SIZE)\n",
        "        self.dropout = nn.Dropout(p=DROPOUT_RATE)\n",
        "        self.layer2 = nn.Linear(HIDDEN_LAYER_SIZE, ACTION_SPACE)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "        action_scores = self.layer2(x)\n",
        "        return F.softmax(action_scores, dim=1)\n",
        "\n",
        "\n",
        "policy = Policy()\n",
        "# Create our policy object\n",
        "\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "\n",
        "eps = np.finfo(np.float32).eps.item() \n",
        "# Machine epsilon, the smallest number computable, is used to avoid division by zero\n",
        "\n",
        "def select_action(state):\n",
        "\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0) \n",
        "    # unsqueeze: Returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "    # Makes the state a tensor of appropriate dimensions\n",
        "\n",
        "    probs = policy(state) \n",
        "    # Passing the state (in tensor form) to the Policy, our neural net, which returns the probs of the two actions\n",
        "    \n",
        "    m = Categorical(probs) \n",
        "    # Creates a categorical distribution parameterized by probs (0 -> prob(0), 1 -> prob(1))\n",
        "\n",
        "    action = m.sample()\n",
        "    # Chooses an action according to the probs\n",
        "\n",
        "    policy.saved_log_probs.append(m.log_prob(action)) \n",
        "    # torch.distributions.Categorical contains the function log_prob(something), we're calling it here\n",
        "\n",
        "    return action.item() \n",
        "    # Use torch.Tensor.item() to get a Python number from a tensor containing a single value\n",
        "\n",
        "def finish_episode():\n",
        "    R = 0 # For total rewards\n",
        "    policy_loss = []\n",
        "    returns = []\n",
        "\n",
        "    for r in policy.rewards[::-1]: \n",
        "        # Goes through rewards in inverse order, calculates discounted rewards\n",
        "\n",
        "        R = r + gamma * R \n",
        "        # Total reward in s(t) = reward in s(t) + gamma * total rewards in s(t+1)\n",
        "\n",
        "        returns.insert(0, R) \n",
        "        # The list insert() method inserts an element to the list at the specified index.\n",
        "\n",
        "    returns = torch.tensor(returns)\n",
        "    # Make it a tensor\n",
        "\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps) \n",
        "    # Z-score = - mean / std. How far away are they from the mean?\n",
        "    # Eps is added to avoid division by zero\n",
        "\n",
        "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
        "        policy_loss.append(-log_prob * R) \n",
        "        # Multiply log probs times returns, minus since we want to maximize returns\n",
        "\n",
        "    optimizer.zero_grad() \n",
        "    # Set the gradients to zero so they don't accumulate\n",
        "\n",
        "    policy_loss = torch.cat(policy_loss).sum() \n",
        "    # cat: Concatenates the given sequence of seq tensors in the given dimension. Then .sum() sums them\n",
        "\n",
        "    policy_loss.backward()\n",
        "    # Update grads: x.grad += dloss/dx\n",
        "\n",
        "    optimizer.step() \n",
        "    # Update params. For SGD would be x += -lr * x.grad\n",
        "\n",
        "    del policy.rewards[:]\n",
        "     # Delete rewards, we don't need those anymore\n",
        "\n",
        "    del policy.saved_log_probs[:] \n",
        "    # Delete saved_log_probs, we don't need those anymore\n",
        "\n",
        "def main():\n",
        "    running_reward = 10\n",
        "    for i_episode in count(1): \n",
        "        # Run as many episodes as needed, faster than other loops\n",
        "\n",
        "        state, ep_reward = env.reset(), 0 \n",
        "        # Reset the environment and the reward to 0\n",
        "\n",
        "        for t in range(1, 10000): \n",
        "            # Don't infinite loop while learning, maximum ep length: 10'000\n",
        "\n",
        "            action = select_action(state) \n",
        "            # Pass the state to the neural network, returns chosen action\n",
        "\n",
        "            state, reward, done, _ = env.step(action) \n",
        "            # Typical gym stuff, take the action, returns state, reward, done and info (not needed, hence _)\n",
        "\n",
        "            if render == True:\n",
        "                env.render()\n",
        "\n",
        "            policy.rewards.append(reward)\n",
        "            ep_reward += reward \n",
        "            # Total reward for the episode\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward \n",
        "        # Slowly update the \"average\" reward, so only get there when we get there\n",
        "\n",
        "        finish_episode() \n",
        "        # Updates the params in our Policy\n",
        "\n",
        "        if i_episode % 10 == 0: \n",
        "            # Every 10 episodes print an update\n",
        "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                  i_episode, ep_reward, running_reward))\n",
        "            if running_reward > env.spec.reward_threshold: \n",
        "                # Stop when the \"average\" reward gets over the threshold (475 for CartPole)\n",
        "\n",
        "                print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0iyUSNjvmw1",
        "outputId": "e8394d65-433f-4e93-99e4-6f2b541c054c"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 10\tLast reward: 69.00\tAverage reward: 16.74\n",
            "Episode 20\tLast reward: 23.00\tAverage reward: 24.05\n",
            "Episode 30\tLast reward: 85.00\tAverage reward: 33.88\n",
            "Episode 40\tLast reward: 37.00\tAverage reward: 45.31\n",
            "Episode 50\tLast reward: 16.00\tAverage reward: 59.90\n",
            "Episode 60\tLast reward: 162.00\tAverage reward: 95.08\n",
            "Episode 70\tLast reward: 108.00\tAverage reward: 99.79\n",
            "Episode 80\tLast reward: 277.00\tAverage reward: 133.26\n",
            "Episode 90\tLast reward: 99.00\tAverage reward: 165.36\n",
            "Episode 100\tLast reward: 86.00\tAverage reward: 136.98\n",
            "Episode 110\tLast reward: 18.00\tAverage reward: 127.73\n",
            "Episode 120\tLast reward: 84.00\tAverage reward: 125.35\n",
            "Episode 130\tLast reward: 126.00\tAverage reward: 119.86\n",
            "Episode 140\tLast reward: 462.00\tAverage reward: 156.19\n",
            "Episode 150\tLast reward: 123.00\tAverage reward: 158.56\n",
            "Episode 160\tLast reward: 88.00\tAverage reward: 144.48\n",
            "Episode 170\tLast reward: 176.00\tAverage reward: 151.33\n",
            "Episode 180\tLast reward: 500.00\tAverage reward: 204.67\n",
            "Episode 190\tLast reward: 201.00\tAverage reward: 267.42\n",
            "Episode 200\tLast reward: 176.00\tAverage reward: 235.92\n",
            "Episode 210\tLast reward: 168.00\tAverage reward: 216.98\n",
            "Episode 220\tLast reward: 347.00\tAverage reward: 238.80\n",
            "Episode 230\tLast reward: 500.00\tAverage reward: 334.24\n",
            "Episode 240\tLast reward: 500.00\tAverage reward: 384.92\n",
            "Episode 250\tLast reward: 500.00\tAverage reward: 431.10\n",
            "Episode 260\tLast reward: 500.00\tAverage reward: 455.50\n",
            "Episode 270\tLast reward: 500.00\tAverage reward: 473.35\n",
            "Episode 280\tLast reward: 500.00\tAverage reward: 440.02\n",
            "Episode 290\tLast reward: 200.00\tAverage reward: 392.88\n",
            "Episode 300\tLast reward: 249.00\tAverage reward: 369.11\n",
            "Episode 310\tLast reward: 286.00\tAverage reward: 346.15\n",
            "Episode 320\tLast reward: 141.00\tAverage reward: 270.18\n",
            "Episode 330\tLast reward: 166.00\tAverage reward: 237.29\n",
            "Episode 340\tLast reward: 500.00\tAverage reward: 328.71\n",
            "Episode 350\tLast reward: 500.00\tAverage reward: 394.10\n",
            "Episode 360\tLast reward: 500.00\tAverage reward: 404.28\n",
            "Episode 370\tLast reward: 500.00\tAverage reward: 352.35\n",
            "Episode 380\tLast reward: 131.00\tAverage reward: 278.97\n",
            "Episode 390\tLast reward: 117.00\tAverage reward: 217.24\n",
            "Episode 400\tLast reward: 111.00\tAverage reward: 174.88\n",
            "Episode 410\tLast reward: 500.00\tAverage reward: 209.25\n",
            "Episode 420\tLast reward: 500.00\tAverage reward: 279.84\n",
            "Episode 430\tLast reward: 500.00\tAverage reward: 368.18\n",
            "Episode 440\tLast reward: 500.00\tAverage reward: 413.82\n",
            "Episode 450\tLast reward: 262.00\tAverage reward: 390.97\n",
            "Episode 460\tLast reward: 304.00\tAverage reward: 371.41\n",
            "Episode 470\tLast reward: 105.00\tAverage reward: 314.12\n",
            "Episode 480\tLast reward: 114.00\tAverage reward: 251.12\n",
            "Episode 490\tLast reward: 132.00\tAverage reward: 206.18\n",
            "Episode 500\tLast reward: 115.00\tAverage reward: 168.77\n",
            "Episode 510\tLast reward: 124.00\tAverage reward: 146.83\n",
            "Episode 520\tLast reward: 126.00\tAverage reward: 139.46\n",
            "Episode 530\tLast reward: 128.00\tAverage reward: 132.27\n",
            "Episode 540\tLast reward: 132.00\tAverage reward: 126.54\n",
            "Episode 550\tLast reward: 110.00\tAverage reward: 118.03\n",
            "Episode 560\tLast reward: 96.00\tAverage reward: 115.62\n",
            "Episode 570\tLast reward: 69.00\tAverage reward: 114.62\n",
            "Episode 580\tLast reward: 135.00\tAverage reward: 119.90\n",
            "Episode 590\tLast reward: 326.00\tAverage reward: 155.23\n",
            "Episode 600\tLast reward: 500.00\tAverage reward: 293.57\n",
            "Episode 610\tLast reward: 500.00\tAverage reward: 376.40\n",
            "Episode 620\tLast reward: 500.00\tAverage reward: 415.32\n",
            "Episode 630\tLast reward: 500.00\tAverage reward: 430.41\n",
            "Episode 640\tLast reward: 500.00\tAverage reward: 458.17\n",
            "Episode 650\tLast reward: 199.00\tAverage reward: 378.48\n",
            "Episode 660\tLast reward: 133.00\tAverage reward: 289.14\n",
            "Episode 670\tLast reward: 136.00\tAverage reward: 237.01\n",
            "Episode 680\tLast reward: 500.00\tAverage reward: 287.62\n",
            "Episode 690\tLast reward: 500.00\tAverage reward: 372.84\n",
            "Episode 700\tLast reward: 500.00\tAverage reward: 423.86\n",
            "Episode 710\tLast reward: 500.00\tAverage reward: 454.41\n",
            "Episode 720\tLast reward: 500.00\tAverage reward: 472.71\n",
            "Episode 730\tLast reward: 500.00\tAverage reward: 483.66\n",
            "Solved! Running reward is now 483.65831224263 and the last episode runs to 500 time steps!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-HwyhnevpQg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}